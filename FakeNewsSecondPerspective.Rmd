---
title: "OneForAll"
author: "Anita Kurm, Maris Sala"
date: "May 24, 2018"
output: pdf_document
---

Data Clean up
```{r}
pacman::p_load(caret)

d1 = read.csv("Data/lastoutbreaksize.csv")
d1$ID = 1
d2 = read.csv("Data/lastprobdetect.csv")
d2$ID = 2
d3 = read.csv("Data/SimulationData.csv")
d3$ID = 3
d4 = read.csv("Data/simulationdata1.csv")
d4$ID = 4
d5 = read.csv("Data/simulationdata2.csv")
d5$ID = 5

dataset<-d1
dataset<-rbind(dataset, d2)
dataset<-rbind(dataset, d3)
dataset<-rbind(dataset, d4)
dataset<-rbind(dataset, d5)

# Change column names
colnames(dataset) = c("runNumber", "nodesAmount", "outbreakSize", "probDetect", "recovery", "tick", "turtlesAmount", "clusteringCoef", "pathLength", "naivesAmount", "spreadersAmount", "educatedAmount", "untouchedAmount", "fileID")

# Remove turtlesAmount
dataset$turtlesAmount = NULL

# blue ones = naives
# red = spreaders
# grey = educated
# yellow = untouched

# Order based on file ID so that the data from the same simulation comes together
dataset$runNumber = as.numeric(dataset$runNumber)
dat <- dataset[order(dataset$fileID),]

# Scaled dataframe
#preObj <- preProcess(dat[, c(-1,-14)], method=c("center", "scale"))
#datscaled <- predict(preObj, dat)
# Should not need this because:
#These variables have zero variances: nodesAmount, recovery, turtlesAmount

# Save as a file just in case
write.csv(dat, file = "CleanData.csv")
```



Second clean-up (removing 0.01 probdetect) and standardizing
```{r}
#data
data = read.csv("CleanData.csv")
data$X = NULL

#exclude probdetect 0.01 from the dataset for now
data = subset(data, probDetect > 0.01)
data = droplevels(data)

#new column
data$ID_new = as.factor(paste(data$fileID, data$runNumber, sep = "_"))

#arrange
df<-dplyr::arrange(data, ID_new)

#Summarize by so we know the ticks it took to complete each sim - our outcome
maxTick = dplyr::group_by(df,ID_new) %>%
  dplyr::summarize(maxTick = max(tick))
new_df=merge(df,maxTick, all = T)

rescalelist = c("outbreakSize","probDetect", "clusteringCoef","pathLength")
new_df.s = new_df[, colnames(new_df) %in% rescalelist] %>% 
  lapply(.,function(x) scale(x,center= mean(x,na.rm = T), scale = sd(x, na.rm = T)))%>% 
  cbind(.,new_df[,! colnames(new_df) %in% rescalelist]) 

new_df.s2 = unique(new_df.s$ID_new)
new_df.s2 = group_by(new_df.s, ID_new, outbreakSize, probDetect, maxTick,clusteringCoef, pathLength) %>%
  summarise_each(funs(mean(., na.rm = TRUE)), runNumber)

# Save as a file just in case
write.csv(new_df.s2, file = "NewCleanData.csv")
```


Data visualization (unstandardized for easier interpretation)
```{r}
data = read.csv("CleanData.csv")
data$X = NULL

data$probDetect = as.factor(data$probDetect)

# Without 0.01 condition
d1 = subset(data, outbreakSize == "238" & probDetect != "0.01")
d2 = subset(data, outbreakSize == "3" & probDetect != "0.01")
d3<- subset(data, probDetect != "0.01")
d3$naivesAmount_new <- (d3$naivesAmount - d3$untouchedAmount)

ggplot(d2, aes(tick, spreadersAmount, color = probDetect)) +
  geom_point() +
  theme_classic()+
  ggtitle("Size = 3, Amount of spreaders over time")

ggplot(d1, aes(tick, spreadersAmount, color = probDetect)) +
  geom_point() +
  theme_classic()+
  ggtitle("Size = 238, Amount of spreaders over time")

H1 = ggplot(d3, aes(tick, spreadersAmount, color = outbreakSize)) +
  geom_smooth() +
  #geom_point()+
  theme_classic() +
  facet_grid(~outbreakSize) +
  ggtitle("Spreaders") +
  theme_minimal() +
  theme(legend.position="none")

H2 = ggplot(d3, aes(tick, educatedAmount, color = outbreakSize)) +
  geom_smooth() +
  #geom_point()+
  theme_classic() +
  facet_grid(~outbreakSize) +
  ggtitle("Educated") +
  theme_minimal() +
  theme(legend.position="none")

H3 = ggplot(d3, aes(tick, untouchedAmount, color = outbreakSize)) +
  geom_smooth() +
  #geom_point()+
  theme_classic() +
  facet_grid(~outbreakSize) +
  ggtitle("Untouched") +
  theme_minimal() +
  theme(legend.position="none")

H4 = ggplot(d3, aes(tick, naivesAmount_new, color = outbreakSize)) +
  geom_smooth() +
  #geom_point()+
  theme_classic() +
  facet_grid(~outbreakSize) +
  ggtitle("Naives") +
  theme_minimal() +
  theme(legend.position="none")

gridExtra::grid.arrange(H1,H2,H3,H4)

# model 4
ggplot(d3, aes(clusteringCoef, y = ..density..)) +
  geom_histogram()

ggplot(d3, aes(pathLength, y = ..density..)) +
  geom_histogram()

ggplot(d3, aes(spreadersAmount, y = ..density..)) +
  geom_histogram()

# untouched kids in the end of simulation
library("dplyr")
d6 = d3 %>%
group_by(runNumber) %>%
slice(c(1, n())) %>%
ungroup()
#Get rid of 1st row
maxTick2 = dplyr::group_by(d6,runNumber) %>%
  dplyr::summarize(maxTick2 = max(tick))
d7=merge(d6,maxTick2)
d7 = subset(d7, maxTick2 == tick)

# Distribution of untouched ones in the end of simulations
ggplot(d7, aes(untouchedAmount, y = ..density..)) +
  geom_histogram() # very odd distribution

# Untouched final amounts for each run number
d8 = subset(d7, untouchedAmount > 0)
ggplot(d8, aes(runNumber, untouchedAmount)) +
  geom_point() # Not gonna use this

### PROPER VISUALIZATIONS FOR THE TWO MODELS ###
## Based on new.df.s2 but before scaling it because it is the same dataset we use for the models
#Summarize by so we know the ticks it took to complete each sim - our outcome
and = unique(new_df$ID_new)
and = group_by(new_df, ID_new, outbreakSize, probDetect, maxTick,clusteringCoef, pathLength) %>%
  summarise_each(funs(mean(., na.rm = TRUE)), runNumber)

# For the first model
and$probDetect = as.factor(and$probDetect)
ggplot(and, aes(outbreakSize, maxTick, color = probDetect)) +
  geom_point() +
  facet_grid(~probDetect) +
  theme_minimal() +
  theme(legend.position="none") +
  labs(x = "Outbreak size", y = "Maximum amount of tick per simulation")

# For the second model
and$outbreakSize = as.factor(and$outbreakSize)
ggplot(and, aes(clusteringCoef, maxTick, color = outbreakSize)) +
  geom_point() +
  facet_grid(~outbreakSize) +
  theme_minimal() +
  theme(legend.position="none") +
  labs(x = "Clustering coefficient", y = "Maximum amount of tick per simulation")

ggplot(and, aes(pathLength, maxTick, color = outbreakSize)) +
  geom_point() +
  facet_grid(~outbreakSize) +
  theme_minimal() +
  theme(legend.position="none") +
  labs(x = "Average path length", y = "Maximum amount of tick per simulation")

```


Analysis.
Models:
```{r}
#Look at the outcome function - which likelihood function do we want?
  #Outcome is count - poisson 
dens(new_df.s2$maxTick)
mean(new_df.s2$maxTick) #15.8295 
sd(new_df.s2$maxTick)  #9.5139 - intercept prior: normal(log(15.83), log(9.51))

## MODEL 2 ##
#ProbDetect model
m2_formula <- bf(maxTick ~ outbreakSize + probDetect)

get_prior(m2_formula,new_df.s2) #Asking the model which priors it recommend

prior = c(prior(normal(log(15.83),log(9.51)), class = Intercept),
          prior(normal(0,0.5), class = b, coef = outbreakSize), 
          prior(normal(0,0.5), class = b, coef = probDetect))


m2 <- brm(m2_formula,
          family = poisson(link = "log"), #We assume our likelihood function to be poisson
          prior = prior, #our list of pre-defined priors
          data = new_df.s2,
          warmup = 4000, # increased number of warmup and iterations
          iter = 10000,
          cores = 3,
          chain = 3)

summary(m2)
plot(m2)

## MODEL 4 ##
#Structure effects
m4_formula <- bf(maxTick ~ clusteringCoef*pathLength)


get_prior(m4_formula,new_df.s2) #Asking the model which priors it recommends 

prior = c(prior(normal(log(15.83), log(9.51)), class = Intercept),
          prior(normal(0,0.5), class = b, coef = clusteringCoef),
          prior(normal(0,0.5), class = b, coef = pathLength),
          prior(normal(0,0.5), class = b, coef = clusteringCoef:pathLength))
        

m4 <- brm(m4_formula,
          family = poisson(link = "log"), #We assume our likelihood function to be normally distributed
          prior = prior, #our list of pre-defined priors
          data = new_df.s2,
          iter = 10000,
          warmup = 4000,
          cores = 3,
          chain = 3)

summary(m4)
plot(m4)
```


Visualize priors, posteriors
```{r}
#Plot priors
x <- seq(-2,2, length=1e5)
y <- dnorm(x, 0, 0.5)
y.I <- dnorm(x, log(15.83), log(9.51))

prior_df <- data.frame(x = rep(x,1), y = c(y), prior = c(rep("Beta Prior", length(y))))
p5=ggplot(prior_df, aes(x = x, y = y, color = prior)) + geom_line() + ggtitle("Beta Prior")+theme_minimal()+theme(legend.position="none")

x <- seq(0,4, length = 1e5)
prior_df <- data.frame(x = rep(x,1), y = c(y.I), prior = c(rep("Intercept Prior", length(y.I))))
p4=ggplot(prior_df, aes(x = x, y = y, color = prior)) + geom_line() + ggtitle("Intercept Prior")+theme_minimal()+theme(legend.position="none")

#plot posterior distributions 
  #Model 1
post_samples <- c(posterior_samples(m2)$b_Intercept)
post_df <- data.frame(post_samples = post_samples, parameter = c(rep("Intercept", 1000)))
p1 = ggplot(post_df, aes(x = post_samples, color = parameter)) + geom_density(adjust = 1) + labs(title = "Model 1: Posterior Distribution - Intercept", x = "Posterior Samples")+theme_minimal()+theme(legend.position="none")

post_samples <- c(posterior_samples(m2)$b_outbreakSize)
post_df <- data.frame(post_samples = post_samples, parameter = c(rep("Outbreak Size", 1000)))
p2 = ggplot(post_df, aes(x = post_samples, color = parameter)) + geom_density(adjust = 1) + labs(title = "Model 1: Posterior Distribution - Outbreak Size", x = "Posterior Samples")+theme_minimal()+theme(legend.position="none")

post_samples <- c(posterior_samples(m2)$b_probDetect)
post_df <- data.frame(post_samples = post_samples, parameter = c(rep("Probability of detection", 1000)))
p3 = ggplot(post_df, aes(x = post_samples, color = parameter)) + geom_density(adjust = 1) + labs(title = "Model 1: Posterior Distribution - Probability of detection", x = "Posterior Samples")+theme_minimal()+theme(legend.position="none")

gridExtra::grid.arrange(p4,p5)
gridExtra::grid.arrange(p1,p2,p3)

```


Model 4
```{r}
#Model 4
post_samples <- c(posterior_samples(m4)$b_Intercept)
post_df <- data.frame(post_samples = post_samples, parameter = c(rep("Intercept", 1000)))
m1=ggplot(post_df, aes(x = post_samples, color = parameter)) + geom_density(adjust = 1) + labs(title = "Model 2: Posterior Distribution - Intercept", x = "Posterior Samples")+theme_minimal()+theme(legend.position="none")

post_samples <- c(posterior_samples(m4)$b_clusteringCoef)
post_df <- data.frame(post_samples = post_samples, parameter = c(rep("Clustering Coefficient", 1000)))
m3=ggplot(post_df, aes(x = post_samples, color = parameter)) + geom_density(adjust = 1) + labs(title = "Model 2: Posterior Distribution - Clustering Coefficient", x = "Posterior Samples")+theme_minimal()+theme(legend.position="none")

post_samples <- c(posterior_samples(m4)$b_pathLength)
post_df <- data.frame(post_samples = post_samples, parameter = c(rep("Average Path Length", 1000)))
m40=ggplot(post_df, aes(x = post_samples, color = parameter)) + geom_density(adjust = 1) + labs(title = "Model 2: Posterior Distribution - Average Path Length", x = "Posterior Samples")+theme_minimal()+theme(legend.position="none")

gridExtra::grid.arrange(m1,m3,m40)
m40
```

Comparision and quality check
```{r}
# Information criterion (WAIC and weights)
waic <- brms::WAIC(m2, m4)
weights <- brms::model_weights(m2,m4, weights = "waic")
waic
weights

#Predictive posterior
pp_check(m2,nsamples = 200)
pp_check(m4,nsamples = 200)

  #Making sense of estimates - posterior distribtions for estimates
stanplot(m2)
stanplot(m4)
```

