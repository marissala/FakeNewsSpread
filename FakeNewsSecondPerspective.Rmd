---
title: "OneForAll"
author: "Anita Kurm, Maris Sala"
date: "May 24, 2018"
output: pdf_document
---

Data Clean up
```{r}
pacman::p_load(caret)

d1 = read.csv("Data/lastoutbreaksize.csv")
d1$ID = 1
d2 = read.csv("Data/lastprobdetect.csv")
d2$ID = 2
d3 = read.csv("Data/SimulationData.csv")
d3$ID = 3
d4 = read.csv("Data/simulationdata1.csv")
d4$ID = 4
d5 = read.csv("Data/simulationdata2.csv")
d5$ID = 5

dataset<-d1
dataset<-rbind(dataset, d2)
dataset<-rbind(dataset, d3)
dataset<-rbind(dataset, d4)
dataset<-rbind(dataset, d5)

# Change column names
colnames(dataset) = c("runNumber", "nodesAmount", "outbreakSize", "probDetect", "recovery", "tick", "turtlesAmount", "clusteringCoef", "pathLength", "naivesAmount", "spreadersAmount", "educatedAmount", "untouchedAmount", "fileID")

# Remove turtlesAmount
dataset$turtlesAmount = NULL

# blue ones = naives
# red = spreaders
# grey = educated
# yellow = untouched

# Order based on file ID so that the data from the same simulation comes together
dataset$runNumber = as.numeric(dataset$runNumber)
dat <- dataset[order(dataset$fileID),]

# Scaled dataframe
#preObj <- preProcess(dat[, c(-1,-14)], method=c("center", "scale"))
#datscaled <- predict(preObj, dat)
# Should not need this because:
#These variables have zero variances: nodesAmount, recovery, turtlesAmount

# Save as a file just in case
write.csv(dat, file = "CleanData.csv")
```



Second clean-up (removing 0.01 probdetect) and standardizing
```{r}
#data
data = read.csv("CleanData.csv")
data$X = NULL

#exclude probdetect 0.01 from the dataset for now
data = subset(data, probDetect > 0.01)
data = droplevels(data)

#new column
data$ID_new = as.factor(paste(data$fileID, data$runNumber, sep = "_"))

#arrange
df<-dplyr::arrange(data, ID_new)

#Summarize by so we know the ticks it took to complete each sim - our outcome
maxTick = dplyr::group_by(df,ID_new) %>%
  dplyr::summarize(maxTick = max(tick))
new_df=merge(df,maxTick, all = T)

rescalelist = c("outbreakSize","probDetect", "clusteringCoef","pathLength")
new_df.s = new_df[, colnames(new_df) %in% rescalelist] %>% 
  lapply(.,function(x) scale(x,center= mean(x,na.rm = T), scale = sd(x, na.rm = T)))%>% 
  cbind(.,new_df[,! colnames(new_df) %in% rescalelist]) 

new_df.s2 = unique(new_df.s$ID_new)
new_df.s2 = group_by(new_df.s, ID_new, outbreakSize, probDetect, maxTick,clusteringCoef, pathLength) %>%
  summarise_each(funs(mean(., na.rm = TRUE)), runNumber)

# Save as a file just in case
write.csv(new_df.s2, file = "NewCleanData.csv")
```


Data visualization (unstandardized for easier interpretation)
```{r}
data = read.csv("CleanData.csv")
data$X = NULL

data$probDetect = as.factor(data$probDetect)



# Without 0.01 condition
d1 = subset(data, outbreakSize == "238" & probDetect != "0.01")
d2 = subset(data, outbreakSize == "3" & probDetect != "0.01")
d3<- subset(data, probDetect != "0.01")

ggplot(d2, aes(tick, spreadersAmount, color = probDetect)) +
  geom_point() +
  theme_classic()+
  ggtitle("Size = 3, Amount of spreaders over time")

ggplot(d1, aes(tick, spreadersAmount, color = probDetect)) +
  geom_point() +
  theme_classic()+
  ggtitle("Size = 238, Amount of spreaders over time")

ggplot(d3, aes(tick, spreadersAmount, color = outbreakSize)) +
  geom_smooth() +
  #geom_point()+
  theme_classic() +
  facet_grid(~outbreakSize) +
  ggtitle("Spreaders")

ggplot(d3, aes(tick, educatedAmount, color = outbreakSize)) +
  geom_smooth() +
  #geom_point()+
  theme_classic() +
  facet_grid(~outbreakSize) +
  ggtitle("Educated")

ggplot(d3, aes(tick, untouchedAmount, color = outbreakSize)) +
  geom_smooth() +
  #geom_point()+
  theme_classic() +
  facet_grid(~outbreakSize) +
  ggtitle("Untouched")

ggplot(d3, aes(tick, naivesAmount, color = outbreakSize)) +
  geom_smooth() +
  #geom_point()+
  theme_classic() +
  facet_grid(~outbreakSize) +
  ggtitle("Naives")

```


Analysis.
Models:
```{r}
#Look at the outcome function - which likelihood function do we want?
  #Outcome is count - poisson 
dens(new_df.s2$maxTick)
mean(new_df.s2$maxTick) #15.8295 
sd(new_df.s2$maxTick)  #9.5139 - intercept prior: normal(log(15.83), log(9.51))

## MODEL 1 ##
#ProbDetect model
m1_formula <- bf(maxTick ~ outbreakSize + probDetect)

get_prior(m1_formula,new_df.s2) #Asking the model which priors it recommend

prior = c(prior(normal(log(15.83),log(9.51)), class = Intercept),
          prior(normal(0,0.1), class = b, coef = outbreakSize), 
          prior(normal(0,0.1), class = b, coef = probDetect))
          #prior(cauchy(0,2), class = sigma)) 


m1 <- brm(m1_formula,
          family = poisson(link = "log"), #We assume our likelihood function to be poisson
          prior = prior, #our list of pre-defined priors
          data = new_df.s2,
          warmup = 2500, # increased number of warmup and iterations
          iter = 5000,
          cores = 3,
          chain = 3)

summary(m1)
plot(m1)

## MODEL 2 ##
#Structure effects
m2_formula <- bf(maxTick ~ clusteringCoef*pathLength + outbreakSize)


get_prior(m2_formula,new_df.s2) #Asking the model which priors it recommends 

prior = c(prior(normal(log(15.83), log(9.51)), class = Intercept),
          prior(normal(0,0.1), class = b, coef = clusteringCoef),
          prior(normal(0,0.1), class = b, coef = pathLength),
          prior(normal(0,0.1), class = b, coef = outbreakSize),
          prior(normal(0,0.1), class = b, coef = clusteringCoef:pathLength))
          #prior(cauchy(0,2), class = sigma))

m2 <- brm(m2_formula,
          family = poisson(link = "log"), #We assume our likelihood function to be normally distributed
          prior = prior, #our list of pre-defined priors
          data = new_df.s2,
          iter = 5000,
          warmup = 2500,
          cores = 3,
          chain = 3)

summary(m2)
plot(m2)
```

